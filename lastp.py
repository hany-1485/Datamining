# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iif6E-3ylew77cYkh9zP6aBaKTpcCUEf
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score ,precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster


from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.cluster import AgglomerativeClustering

dataset=pd.read_csv("/modified_diabetes_prediction_dataset.csv")
dataset.head()

dataset.shape

columns=dataset.columns
print(columns)

dataset = dataset.dropna(subset=['diabetes'])

# حذف داده‌های تکراری
dataset = dataset.drop_duplicates()

dataset.shape

dataset['smoking_history'].unique()

dataset['gender'].unique()

dataset['gender'].mode()

dataset['smoking_history'].mode()

dataset['gender'] = dataset['gender'].map({'Male':0 ,'Female': 1,'Other':1,'unknown':1})
dataset['smoking_history'] = dataset['smoking_history'].map({'never':0,'former':1,'not current':2,'No Info':3, 'current':4 ,'yes':4, 'ever':5})

dataset['smoking_history'].median()

dataset['smoking_history'].mode()

dataset= dataset.fillna(value={	'gender': dataset['gender'].mode(),'smoking_history': dataset['smoking_history'].mode(),'hypertension': dataset['hypertension'].mode(), 'age': dataset['age'].mean(),'bmi': dataset['bmi'].mean(),'blood_glucose_level': dataset['blood_glucose_level'].mean(),'HbA1c_level': dataset['HbA1c_level'].mean() , 'heart_disease': dataset['heart_disease'].mode()})

dataset.head()

dataset['smoking_history'].unique()

dataset['gender'].unique()

# change values that are less than 0
col_num = ['age', 'bmi','blood_glucose_level','HbA1c_level']
for col in col_num:
    mean_value = dataset[dataset[col] > 0][col].mean()
    dataset[col] = dataset[col].apply(lambda x: mean_value if x <= 0 else x)
col_binarry = ['hypertension', 'heart_disease']
for col in col_binarry:
    mode_value = dataset[dataset[col] >= 0][col].mode()
    dataset[col] = dataset[col].apply(lambda x: mode_value if x < 0 else x)

for att in columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(data=dataset[att])
    plt.title(f"Box Plot for {att}")
    plt.show()

dt = dataset.copy()
num=dt.select_dtypes(include=['number']).columns
num = num.drop({'diabetes','hypertension', 'heart_disease','smoking_history'})
mask = pd.Series(True, index=dt.index)
for i in num:
    max_limit = dt[i].mean() + 3*dt[i].std()
    min_limit = dt[i].mean() - 3*dt[i].std()
    print("max limit of ",i," is ",max_limit, " & min limit of ",i," is ",min_limit)
    mask &= (dt[i] < max_limit) & (dt[i] > min_limit)
new_dataset = dt[mask]

# delete illogical val
index_to_drop = new_dataset[((new_dataset['smoking_history'] > 5) | (new_dataset['hypertension'] > 1) | (new_dataset['heart_disease'] > 1) | (new_dataset['diabetes'] > 1))].index

new_dataset = new_dataset.drop(index_to_drop)

new_dataset.shape

new_dataset.head()

x=np.array(new_dataset.drop('diabetes',axis=1))
y=np.array(new_dataset['diabetes'])

x.shape

y.shape

X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.3, random_state=2)

model_logistic = LogisticRegression()

# آموزش مدل
model_logistic.fit(X_train, Y_train)

model_logistic.score(X_test,Y_test)

y_logistic_predict=model_logistic.predict(X_test)

cm = confusion_matrix(Y_test, y_logistic_predict)

# نمایش ماتریس سردرگمی
print("Confusion Matrix:")
print(cm)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
disp.plot(cmap="Blues")

accuracy = accuracy_score(Y_test, y_logistic_predict)
precision = precision_score(Y_test, y_logistic_predict, average='binary')  # برای دسته‌بندی دودویی
recall = recall_score(Y_test, y_logistic_predict, average='binary')
f1 = f1_score(Y_test, y_logistic_predict, average='binary')

# نمایش نتایج
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

print("\nClassification Report:")
print(classification_report(Y_test, y_logistic_predict, target_names=["Class 0", "Class 1"]))

model_DecisionTree = DecisionTreeClassifier(random_state=42)

# آموزش مدل
model_DecisionTree.fit(X_train, Y_train)

model_DecisionTree.score(X_test,Y_test)

y_DecisionTree_predict=model_DecisionTree.predict(X_test)

con = confusion_matrix(Y_test, y_DecisionTree_predict)

# نمایش ماتریس سردرگمی
print("Confusion Matrix:")
print(con)

showD = ConfusionMatrixDisplay(confusion_matrix=con, display_labels=[0, 1])
showD.plot(cmap="Greens")

accuracy = accuracy_score(Y_test, y_DecisionTree_predict)
precision = precision_score(Y_test, y_DecisionTree_predict, average='binary')  # برای دسته‌بندی دودویی
recall = recall_score(Y_test, y_DecisionTree_predict, average='binary')
f1 = f1_score(Y_test, y_DecisionTree_predict, average='binary')

# نمایش نتایج
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

print("\nClassification Report:")
print(classification_report(Y_test, y_DecisionTree_predict, target_names=["not diabets", "diabets"]))

scaler = StandardScaler()
X_scaled = scaler.fit_transform(x)
# k range
k_values = range(2,7)
silhouette_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(x)
    silhouette_avg = silhouette_score(x, cluster_labels)
    silhouette_scores.append(silhouette_avg)
    print(f"silhouette_score for k={k} : {silhouette_avg:.3f}")

# find optimal_k
optimal_k = k_values[np.argmax(silhouette_scores)]
print(f"best k: {optimal_k}")

scaler = StandardScaler()
X_normalized = scaler.fit_transform(x)

# kMeans
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
new_dataset['KMeans_Cluster'] = kmeans.fit_predict(X_normalized)

# hierarchy
np.random.seed(42)
sample_indices = np.random.choice(len(X_normalized), size=2000, replace=False)
X_sample = X_normalized[sample_indices]

# plot dendrogram
linked = linkage(X_sample, method='ward', metric='euclidean')
plt.figure(figsize=(12, 8))
dendrogram(linked, truncate_mode='lastp', p=30, distance_sort='ascending', show_leaf_counts=True)
plt.title('Truncated Dendrogram')
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_normalized)

clustering = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
clusters = clustering.fit_predict(X_reduced)

# add cluster to dataset
new_dataset['hierarchical_Cluster'] = clusters

# show clustering
pca_2d = PCA(n_components=2)
X_2d = pca_2d.fit_transform(X_reduced)
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=clusters, palette='viridis')
plt.title('Cluster Visualization')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.show()

# kmeans clusters mean
kmeans_clusters_mean = new_dataset.groupby('KMeans_Cluster').mean()
print("KMeans mean of clusters:\n", kmeans_clusters_mean)

# hierarchical clusters mean
hierarchical_clusters_mean = new_dataset.groupby('hierarchical_Cluster').mean()
print("hierarchical clusters mean :\n", hierarchical_clusters_mean)

kmeans_labels = {
    0: 'not diabets',
    2: 'diabets',
    1: 'maybe diabets'
}
new_dataset['KMeans_Label'] = new_dataset['KMeans_Cluster'].map(kmeans_labels)

hierarchica_labels = {
    1: 'maybe diabets',
    0: 'not diabets',
    2: 'diabets'
}
new_dataset['hierarchical_Label'] = new_dataset['hierarchical_Cluster'].map(kmeans_labels)

# show hierarchical_Clustering
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=new_dataset['hierarchical_Label'], palette='Set1')
plt.title('Hierarchical Clustering')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.legend(loc='best')
plt.show()

# show KMeans Clustering
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=new_dataset['KMeans_Label'], palette='viridis')
plt.title('KMeans Clustering')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.legend(loc='best')
plt.show()